% Copyright 2021 Jacopo Maltagliati
%
% Use of this source code is governed by an EUPL-style
% license that can be found in the LICENSE file or at
% https://eupl.eu/1.2/en/.
% A localized copy of this license can be found in the LICENSE.it file or at
% https://eupl.eu/1.2/it/
%
% Portions of this document are subject to different licensing agreements,
% see src/titlepage.tex for details.

\documentclass[a4paper,12pt]{report}
\usepackage[paper=a4paper,margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{float}
%\usepackage{hyperref}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[utf8]{inputenc}
\usepackage{setspace}
\usepackage[english]{babel}
%\usepackage{amssymb}
%\usepackage{amsthm}
%\usepackage{amsmath}
%\usepackage{amstext}
\usepackage{microtype}
\usepackage{csquotes}
\usepackage{bookmark}
\usepackage{xpatch}
\usepackage{listings}
\usepackage{minted}
\usemintedstyle{borland}
\usepackage[backend=biber, sorting=none]{biblatex}
\hypersetup{colorlinks=true, linkcolor=blue, citecolor=blue, urlcolor=blue, pdftitle={Linux as a Real-Time Operating System}}
\addbibresource{all.bib}
\usepackage[color=green,textsize=tiny]{todonotes}
\usepackage{sectsty}
\chapternumberfont{\large}
\chaptertitlefont{\huge}
\renewcommand{\familydefault}{\sfdefault}
\newcommand{\comment}[1]{}
\begin{document}
% Titlepage
\input{titlepage.tex}
\newpage
% Quote
\vspace*{\stretch{1}}
\begin{flushright}
  \itshape
  \begin{tabular}{@{}l@{}}
    In (conflicted) memory of Fulvio Testi 174.\\
    2017-2020\\ JM,MP,MS,MT,AM,MC 
  \end{tabular}
\end{flushright}
\vspace{\stretch{2}}
\newpage
% Toc
\tableofcontents
\newpage
% Set paragraph spacing so our eyes don't pop while reading
\setlength{\parskip}{1em}
% Ch0
\chapter*{Acknowledgements}
A special thanks for the continued support and previous work on Otto, ROS and the USAD platform goes to Domenico Sorrenti, Federica Di Lauro, Simone Fontana, Carlo Radice, Giovanni Capra and Riccardo Curnis of the IRAlab team, and to the former IRAlab member Augusto Luis Ballardini.

% Ch1
\chapter{Summary}

In the past ten years, mobile robotics as a field has been gaining a lot of traction: as technology evolves, building a mobile platform has become more and more feasible for anyone, including small research teams and hobbyists. However, there is still a lack of easily approachable real-time solutions able to manage the workload, as many of the available frameworks are not mature enough to offer fine-grained real time application control: this is why we ultimately decided to push the Linux kernel to its limits, studying its real-time capabilities and ease of use.

\section{The problem}

Imagine driving on an empty road at night while listening to classical music: it's been a very long day, and your consciousness gradually drifts to a peaceful slumber. Suddenly, an animal crosses the road: are you going to notice? Will you be able to stop in time? Now, consider a self-driving car in the same situation: what would happen if the control computer "dozed off" even for a fraction of a second? The user would probably not be able to stomp on the brakes fast enough to prevent the car from crashing.

A possible solution to this problem is carefully designing a system built around existing technologies, such as real-time control, redundant hardware platforms, and failsafe mechanisms. However, creating a real-time system from the ground up tends to be very expensive and error-prone, as incorrect design or minor mistakes could lead to catastrophic failures such as the infamous Therac \cite{therac-25-accidents} incidents.

For this reason, many software houses have been working on various commercial solutions since the 1980s that enabled programmers and engineers to deploy their application on proven grounds, thus reducing the time and effort needed to create a real-time application. Arguably, the best aspect of those systems was their widespread availability: for example, DEC's VAXeln used lightly modified VAX computers, and Quantum Computer Inc. QNX ran on consumer-grade hardware, including x86 processors. While the VAXeln is now a historical platform, QNX is still commercially available from BlackBerry Limited and major companies such as Apple use it for their products \cite{time-carplay-qnx}.

However, having always demonstrated an incredible flexibility, the ubiquitous Linux kernel is becoming a viable option for robotics and real-time usage, with both academic and commercial applications exploiting it like Tesla Motors, which maintains its own fork \cite{gh-tesla-linux}.

Of course, this flexibility comes at the cost of reduced effectiveness in each specific situation, so why do both academic and commercial researchers decide to use Linux? For example:
\begin{itemize}
  \item Linux is free and comes at no cost.
  \item Linux is open-source, so it might be tailored to specific needs with the right expertise.
  \item Linux has a wide community of people constantly improving it, ensuring constant updates without any support contract.
\end{itemize}

These, along with the fact that IRAlab has been running Linux on control systems for several years, are the reasons why we decided to adopt it for this project.

\section{The current implementation}

At the moment, most of IRAlab's mobile platforms are controlled by a master node running Ubuntu Linux 16.04 LTS and the ROS framework, which was created by Willow Garage as a rapid development platform for their PR2 robot and traces its roots into an even earlier effort by a team of researchers and students at Stanford University. 

The name "Robot Operating System" is misleading: in fact, ROS is mostly an open-source middleware, that is a set of software libraries and tools that help you build robot applications, which actually needs an underlying operating system to run: currently, it may do so on both Unix-like operating systems such as Ubuntu GNU/Linux, BlackBerry QNX \cite{qnx-ros} and Apple macOS, and on some Microsoft Windows versions. For simplicity, throughout this report we'll use the term "ROS" to refer to the ROS+Linux combination.

\subsection{Description}

Currently, IRAlab mainly uses a modified ROS navigation stack for abstraction and rapid prototyping: through the use of \textit{topics} and \textit{messages}, ROS provides a facility to let the users write small programs, called \textit{nodes}, and make them communicate with each other. A \textit{master node}, provided by the framework, acts as a central hub, keeping tabs on the state of the \textit{Computational Graph}. Moreover, ROS provides extensive \textit{timestamping} and data logging facilities, letting the user easily track, monitor and simulate a mobile platform's behavior.

An in-depth analysis of the ROS functionalities and usage in the context of our mobile platforms is beyond the scope of this report, but the same topic has received extensive coverage in earlier works by Gerosa* and Di Lauro \cite{fdila-bs-otto}.
\todo{dilauro -> ???}

\subsection{Issues}

The problem we encountered with ROS is its complexity: while it's easily modifiable and extensible, there is no simple way to control precisely how the nodes are scheduled. Finely tuning this behavior would imply heavily modifying the codebase, which is a very complex task.

%%???
% Moreover, the presence of a Master node and an inter-node communication protocol based on handshaking (much akin to how MTTQ operates)\todo{cite?} makes it difficult to precisely control timing, unless the whole node setup is performed offline.

Ultimately, some parts of ROS are written in Python \cite{roswiki-rospy}, which is an interpreted (or rather JIT-compiled) language. Python is notoriously hard to control in a real-time environment since it uses facilities like garbage collection \cite{python-devguide-gc} and dynamic allocation that are not designed to obey real-time constraints. The easiest way to avoid this problem is to use a language such as C, which gives the programmer more control over the timing and scheduling of the threads.

An attempt to solve this problem has been made by \textcite{rt-ros-approach}, but we've deemed it as an unnecessarily complex workaround and decided to explore different solutions.

\section{A possible upgrade path}

To develop a complex application cost-effectively, using some scaffolding is necessary to avoid spending more time designing your software than implementing it: this is why the ROS framework, which doubles as a rapid prototyping platform, has gained so much traction in the last decade. 

However, there are several common pitfalls associated with them: increasing modularity inevitably leads to more complex runtimes that must be bundled with your application and cannot be easily modified. While a modular design should address this problem, a fundamental design decision taken by the developers of the framework that clashes with the needs of your application would force you to move to a different platform entirely.

It's impossible to know whether or not such a solution would be suitable for your application beforehand without doing some research: this is why we've decided to study the innards of the ROS 2 framework, which seemed like a viable upgrade path.

\subsubsection{Robot Operating System, version 2}

ROS version 2 is the new generation of the ROS platform which was launched in 2015, and has reached its seventh release. The designers of ROS 2 put more focus on the real-time needs of the robotics industry in an attempt to solve the shortcomings of ROS \cite{ros2-realtime-intro}, as it pipelines tasks in a very different way and uses an enterprise-grade communications subsystem that is much more RT-friendly \todo{cite}.

Keeping the ROS framework would've enabled us to benefit from real-time improvements without the burden of porting our whole navigation stack to an entirely new platform, thus reducing turnaround time significantly.

After some research, however, we've determined that using ROS 2 would not let us control the scheduling precisely enough, making it impossible to control the timing of a specific portion of code within a ROS 2 task with an RT-scheduler provided by Linux such as \texttt{RT} or \texttt{DEADLINE}. This limitation stems from the architecture of the scheduler used in ROS 2: instead of spawning one thread for each task, the framework uses one or more "super-threads", where a custom scheduling algorithm is in charge of scheduling a variable number of tasks. While this design should make the whole system behave in a more deterministic way compared to its predecessor, it's ultimately even more opaque to the underlying operating system (and to the programmer, for that matter) than the one that was used before.

\newpage
% Ch2
\chapter{Solution}

Our custom solution, named \texttt{rt-app}, is a demonstration of what we believe is a viable approach for building real-time control systems on Linux. Written from scratch, the demo currently leverages several APIs provided by Linux such as \texttt{sched(7)} \cite{man-sched-7}, \texttt{time(7)} \cite{man-time-7} and \texttt{pthreads(7)} \cite{man-pthreads-7}.

The main goal of \texttt{rt-app}, however, is demonstrating that replicating a ROS-like behavior is possible by only using facilities provided by the Linux kernel and the GNU standard library: thus, in the following sections, we'll focus more on the implementation rather than on the design process.

\section{Planning}

Given that implementing a complex solution like this in such a short timeframe is impracticable, we've settled for splitting the work in several phases:
\begin{itemize}
  \item Phase 1: High-level design and survey of the possible solutions
  \item Phase 2: Implementation of the dispatcher, at least one of the modules, and integration with an existing mobile platform.
  \item Phase 3: Maintenance and gradual addition of more modules, according to the programming guidelines and general architecture set by the work performed in phase 2.
\end{itemize}

Thus far, Phase 1 and a consistent portion of Phase 2 have been completed: the Odometry module has been implemented, on top of a simple dispatcher based on \texttt{pthreads} and \texttt{SCHED\_EDF}. We've selected IRAlab's Otto \cite{fdila-bs-otto} mobile platform as a test candidate because of its convenient size and ease of maintenance, and we've developed an adapter to interface the Odometry module with the STM32 microcontroller responsible for driving Otto's motors and encoders through the serial port.

\section{Design}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{img/overview.pdf}
    \caption{High-level overview of the design proposal for our application framework}
\end{figure}

\texttt{rt-app}'s design is based around some components considered a standard in the mobile robotics field, such as:
\begin{itemize}
    \item The \textit{pose}, which is the basic unit of data containing all of the information about the orientation and speed of the mobile platform, plus timestamping.
    \item The \textit{odometry}, which constantly elaborates the data received from various sources, such as the mobile platform's wheel encoders, to keep track of where the robot is going.
    \item The \textit{costmap}, which is responsible for determining how likely the robot is to collide with an obstacle, using a predefined map and integrating data from other sources.
    \item The \textit{local planner}, which drives the robot towards a goal using the data provided by the odometry and various other sources, such as AMCL.
    \item \textit{AMCL}, which is a collection of algorithms that use data coming from the sensors, such as a LiDAR, to greatly improve the quality of the localization \cite{roswiki-amcl}.
    \item The \textit{global planner}, which builds a global plan and the costmap from raw data on startup.
\end{itemize}

Moreover, we've envisioned the following components:
\begin{itemize}
  \item The \textit{pose manager}, which is a pivotal component that manages the lifecycle of a pose, controls the flow of data between the modules, and makes sure that timestamping is consistent.
  \item The \textit{dispatcher}, which takes care of initializing threads and setting up shared memory, and is responsible for terminating the application once all workers have finished their job.
\end{itemize}

Most of these are considered on-line operations, as they operate in a real-time environment, except for global planning and thread dispatching, both of which happen once when the execution starts.

\subsection{Kernel}

An integral part of our design revolves around exploiting the capabilities of a real-time operating system: in the Linux world, this can be accomplished through co-kernel solutions, like Xenomai or RTAI, or via the \texttt{PREEMPT\_RT} patch.

The \texttt{PREEMPT\_RT} patch was introduced in 2005 to reduce the latency and increase the determinism of the Linux kernel. Since its inception, it has slowly gained traction amongst the kernel developers' community, and it's hopefully going to be merged into mainline soon \cite{lwn-rt-future}. Given that the co-kernel approach requires an application to use custom APIs, we've chosen to patch our kernel: this ensures that any program written and compiled on off-the-shelf Linux distributions will run unmodified on a patched kernel and benefit from it.

The goal of the patch is not to make the system faster but rather to eliminate virtually all of the mechanisms that introduce unbounded latencies in kernel space. By doing this, the RT kernel can improve the consistency of the system calls significantly \cite{dmoceri-benchmarking-rtlinux}, albeit with a moderate performance hit. A formal model of the Linux kernel latencies and a tool to analyze them have been developed by \textcite{demistifying-rt-latency}.

Nowadays, the impact of \texttt{PREEMPT\_RT} may not be immediately obvious, as many of its features have trickled down from the patch to the mainline kernels over the last 16 years. The most notable examples, from an academic standpoint, are:

\begin{itemize}
    \item The \textit{priority inheritance} system, which attempts to solve the well-discussed problem of priority inversion.
    \item The introduction of \textit{threaded interrupt handlers}, which are necessary to reduce unbounded latencies associated with non-deterministic peripherals such as HIDs.
    %\item The 
\end{itemize}

However, a small portion of the improvements and most configuration options offered by the patch still elude the mainline: this is why to appreciate its benefits, the kernel must be patched, reconfigured, and rebuilt from source. While this operation is straightforward, it can become very time-consuming, as fully rebuilding a kernel on commodity hardware may take several hours.

A thorough analysis of the topics we've briefly covered over this section has been performed by \textcite{survey-preempt-rt}, and it's been instrumental in aligning our expectations with the actual capabilities of a \texttt{PREEMPT\_RT} patched kernel.

\subsection{Scheduler}

The other fundamental part of a real-time system is the scheduler: since kernel 2.6.22, when CFS replaced the older O(1) scheduler as the default scheduler \cite{lwn-cfs-merge}, Linux offers a complex and modular scheduling system based on several independent priority classes. 

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{img/sched-class.pdf}
    \caption{The relationship between the scheduling classes provided by the Linux kernel}
\end{figure}

In each priority class, there may be several different \textit{scheduling policies} that a programmer or a system administrator might choose for a specific workload. In brief: 

\begin{itemize}
    \item \texttt{STOP}: This is the highest priority class, and it's used in kernel space to pin portions of code on specific cores in an SMP setting.
    \item \texttt{DEADLINE}: This class is mostly used to schedule periodic tasks with an EDF algorithm, which is well suited to real-time workloads, such as video decoding and real-time process control.
    \item \texttt{RT}: This is a POSIX standard compatible scheduling class that is used for some latency-sensitive tasks.
    \item \texttt{CFS}: This is the class that's commonly used for all the processes in userspace, and it's the one that uses the \texttt{nice}ness mechanism.
    \item \texttt{IDLE}: This class is responsible for keeping a CPU idling when there are no schedulable tasks left in any queue.
\end{itemize}

For a more detailed description of the scheduling classes, refer to \texttt{sched(7)} \cite{man-sched-7}.

\subsubsection{The \texttt{SCHED\_DEADLINE} scheduling policy}

%% brief history of sched-edf and who's behind it
\texttt{DEADLINE} is a scheduler developed by Dario Faggioli et al. \cite{lwn-faggioli-mail} that's been available in Linux since kernel 3.14 \cite{kn-linux-314}. This scheduler allows the programmer to define and schedule tasks using an Earliest Deadline First (EDF) algorithm, combined with a mechanism called Constant Bandwidth Server (CBS) to perform resource reservation \cite{cbs-algorithm}.

From a programming standpoint, \texttt{SCHED\_DEADLINE} exposes a rich interface to the scheduler that a developer can use to tune the behavior and constraints of a real-time task. The most fundamental parameters that we can tune are: 

\begin{itemize}
    \item The \textit{period}, or the interval at which the scheduler will allow the task to run again after it yielded.
    \item The \textit{deadline}, or the time after which the task has taken too long to complete, and the application must take action.
    \item The \textit{runtime}, or the task's expected worst-case execution time.
\end{itemize}

The EDF algorithm is known in the literature for its reliability and ability to fully utilize the system's computational resources, making it ideal for our demo's purposes.

\section{Implementation}

Due to the nature and the time constraints of this project, implementing the whole application would have been impossible: thus, we've decided to focus on a specific subset of functionalities fundamental for a robot that needs to move and avoid obstacles autonomously. 

Throughout the development of the demo, we've given more attention to the fundamental research needed to steer the project in the right direction and on the real-time aspects of the framework than on the actual payload: as such, the accuracy of the navigation algorithms is not exactly top-notch.

\subsection{Components}

From a software engineering standpoint, there are three distinct kinds of components:
\begin{itemize}
    \item Modules: a header and a source file that contain the schedulable units of the application. Each module contains at least an entry point function executed upon scheduling and some other functions necessary to perform specific tasks.   
    \item Helpers: headers that provide inline functions that wrap around system calls and kernel facilities in a standard way.
    \item Adapters: portions of code that provide a high-level interface to a hardware platform like Otto.
\end{itemize}

The only exceptions to this rule are the \texttt{main.c} file, which provides a scheduling facility, and the \texttt{config.h} file, which contains preprocessor definitions used to change the behavior of the application for debugging purposes.

A permanent link to the current version (as of June 2021) of \texttt{rt-app}'s codebase is available at\todo{link github repo}: the code and the documentation automatically generated by Doxygen at compile time are considered complementary to this report.

\subsection{Threads and scheduling}
Each module contains a schedulable unit, a portion of code that is capable of running in a separate thread, and several scheduling parameters for a \texttt{sched\_setattr()} call to modify the scheduling attributes of the associated thread.

This flexible architecture lets the programmer control how the system will schedule a specific module and by using which algorithm. When using \texttt{SCHED\_DEADLINE}, for example, the programmer can provide a \textit{deadline}, a \textit{runtime} and a \textit{period}: the scheduler will use those parameters to execute the thread appropriately and to know after which amount of time, at worst, it should yield.

To change those parameters, the programmer must define a \texttt{sched\_attr} structure containing some information such as what policy to use, how to react to certain events, and parameters specific to each algorithm, such as a deadline.

\begin{listing}[H]
\inputminted[frame=single,framesep=10pt]{c}{snippets/sched-attr.c}
\caption{Example of the \texttt{sched\_attr} structure for a task with a deadline of 11ms, and a period of 20ms.}
\end{listing}

The \texttt{SCHED\_FLAG\_DL\_OVERRUN} tells the scheduler that we want it to generate a \texttt{SIGXCPU} signal when it detects a deadline miss: this is useful because we can define a recovery behavior that extends or overrides the default one.

\begin{listing}[H]
\inputminted[frame=single,framesep=10pt]{c}{snippets/dl-miss-handler.c}
\caption{Example of a deadline miss handler.}
\end{listing}

Lastly, the thread must perform a \texttt{sched\_setattr()} call to tell the scheduler that we want to use different scheduling parameters than the default ones: if the user running the program has the appropriate rights and no errors occur, the kernel will migrate the task to \texttt{DEADLINE}'s ready queue for execution.

\begin{listing}[H]
\inputminted[frame=single,framesep=10pt]{c}{snippets/entry-point.c}
\caption{Example of a periodic task scheduled with \texttt{SCHED\_DEADLINE}.}
\end{listing}

%https://lwn.net/Articles/743946/

\subsection{Timing}

As usual with systems interacting with hardware sensors, timing is a critical part of our demonstration: a simple timing system, wrapping the \texttt{clock\_*()} family of system calls, has been implemented to readily provide information such as global time deltas in nanoseconds that are useful for timestamping information coming from the adapters. This collection of inline functions and globals is a helper and resides in the \texttt{h\_time.h} header. 

The x86\_64 (AMD64) architecture provides access to a series of hardware clocks via the \texttt{RDTSC} and \texttt{RDTSCP} instructions, which the operating system can use to keep track of the time. These instructions allow the kernel to query the \texttt{TSC} (Time Stamp Counter) internal register to provide us with a reliable hardware time source. However, Intel introduced the \texttt{TSC} register back in the days of the Pentium architecture, and with the advent of technologies like out-of-order execution, hibernation, and multi-core processing, it is not advisable to solely rely on it anymore. This problem, and several possible solutions, have been discussed both in official documentation \cite{intel-rdtsc-bench} and articles from notable sources \cite{ms-rdtsc-issues}.

Conveniently, the Linux kernel works around this problem for us: to accurately advance its internal timers, the kernel uses several clock sources and a unit of measurement called \textit{jiffy}, which is tied to the value of \texttt{HZ} and is fine-tuned to each architecture's needs \cite{elinux-hrts}. Thanks to this complex architecture, which is described in depth by the manual pages \cite{man-clock-getres-2}, we can trust the values provided by system calls such as \texttt{clock\_gettime()} because they're consistent enough to give us usable values in the tens of nanoseconds range.

Besides keeping a time base and calculating deltas, the \texttt{h\_time} helper mostly performs conversions: since the standard library uses \texttt{timespec} structures to pass values around, \texttt{h\_time} is also responsible for translating \texttt{timespec}s to nanoseconds where needed.

\begin{listing}[H]
\inputminted[frame=single,framesep=10pt]{c}{snippets/time.c}
\caption{Example demonstrating the usage of \texttt{clock\_gettime()}, and the conversion of a \texttt{timespec} to nanoseconds.}
\end{listing}

\subsubsection{Performance and average latency}

To measure the performance of our timing system, we've developed a small test suite: exploiting the same design used for the Odometry module, a thread performing 100 die rolls is scheduled with a period of 20ms and a deadline of 11ms. At first, we used \texttt{RDRAND}, which takes a fair amount of clock cycles,  to simulate the die rolls, but the lack of said instruction on Intel® Core™ processors based on the Sandy Bridge (2nd gen) architecture forced us to go back to a more traditional, but less intensive, \texttt{srand()}/\texttt{rand()} approach.

Before each die-rolling cycle, the \texttt{CLOCK\_MONOTONIC} timer is queried with the method described above: this gives us a fair idea of how much skew there is between the expected 20ms interval between each iteration and the time it took the kernel to run our code.

\begin{listing}[H]
\inputminted[frame=single,framesep=10pt]{c}{snippets/latency-test.c}
\caption{By running this payload with a specific scheduler, useful data about the average jitter in different conditions can be gathered.}
\end{listing}

\todo{present the results (aka perform more tests before 31/05!)}

\subsection{Logging}

The problem of logging arose while trying to debug an issue we had with the USAD mobile platform earlier this year: the system experienced a crash, and we were not able to pinpoint a cause because the navigation stack did not use the logging facilities provided by the operating system, but was relying on a custom solution. For this reason, it would've been difficult to precisely determine when a specific error happened in the time domain of the whole system, even if the crash didn't wipe these application logs completely.

To solve this problem, we've decided to use \texttt{syslog()}, which is the standard way to implement logging in Linux: this facility is well documented and comparably more performant than using printf() since there's no data actively written to the console.

\begin{listing}[H]
\inputminted[frame=single,framesep=10pt]{c}{snippets/syslog-example.c}
\caption{Example demonstrating how a program might use \texttt{syslog()} to push warning messages to the global system log.}
\end{listing}

By far, the true benefit of using \texttt{syslog()} lies in the design of the Linux logging system itself: everything that's written to \texttt{/var/log/syslog} has a timestamp and, when the file reaches a set size, the logging daemon backs up the old data for later analysis before overwriting it, and powerful tools like \texttt{journalctl} let a system administrator interactively sift through the logs even if recovered from a local or remote backup.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{img/journalctl.jpeg}
    \caption{Demonstration of the usage of \texttt{journalctl} for log analysis.}
\end{figure}

\todo{journalctl screenshot}

\subsection{Communications}

As we stated earlier, the \textit{adapters} provide a clean way to interact with various peripherals: an adapter is essentially an implementation of a standard interface between a module and a peripheral that wraps around the lengthy set-up portion and system calls a programmer would use to access the hardware.

\subsubsection{Communicating with the Otto mobile platform.}

Presently, the demo application issues command to the Otto mobile platform via an FTDI UART using serial communications at 9600 Baud. The \texttt{a\_otto} adapter provides functions that wrap around the \texttt{read()} and \texttt{write()} system calls to communicate with Otto's STM32 microcontroller over the serial line. All the data passed through the serial port is encoded and decoded using the NanoPB library, an efficient implementation of Google's Protocol Buffers. A detailed description of Otto's communication protocol and its original implementation has been given by \textcite{fdila-bs-otto}.

Getting serial communications to work reliably with our setup was problematic: it's impossible, given Otto's current design, to synchronize the computer and the microcontroller in a way that guarantees us that data will pass through the serial port while the Odometry module's thread is executing. Using a blocking \texttt{read()} would solve this problem, but it would introduce an unbounded latency: the only way around this issue is using a non-blocking call in combination with some buffering.

\begin{listing}[H]
\inputminted[frame=single,framesep=10pt]{c}{snippets/otto-buffering.c}
\caption{Double buffering is necessary to make sure that no bytes are lost in-flight between the kernel and the Odometry algorithm.}
\end{listing}

\section{Future Development}
While a good portion of the groundwork has been completed, the demo in its current state is just a little more than a proof of concept: to make it a usable product for controlling mobile platforms, some "bona fide" interfaces must still be standardized.

The most critical parts that we've yet to define are a system to track the creation of threads, which is currently done rather crudely in \texttt{main()}, and a standard mechanism for inter-module communication, which is probably one of the most fundamental aspects of such a system.

Some other ideas that we've not been able to pursue are:

\begin{itemize}
    \item The possibility of adding adapters that work on sockets instead of hardware devices, to extend the framework to a multi-node scenario;
    \item The development of a benchmark for inter-thread communication primitives in a real-time scenario, that would let us measure the impact of each locking mechanism (semaphores, mutexes, pipes, et cetera) on latency and processing time.
\end{itemize}

Those delicate topics need careful planning and testing before a set of standard interfaces can be defined.

\newpage
% Ch3
\chapter{Implementation Issues}
\todo{unfinished chapter}

While developing this demo, we've encountered some issues that, while not integral to the purpose of this document, have had a notable impact on the design and implementation process of the application: keeping those out of this report wouldn't have been fair, so we've decided to include them in this appendix. 

\section{Patching and building the kernel}

Due to the fact that the preempt-rt patch has still not been mainlined, it was necessary to rebuild the kernel. 

http://kernel-notes.gbittencourt.net/compiling-preempt-rt/

\begin{itemize}
    \item Download https://git.kernel.org/pub/scm/linux/kernel/git/rt/linux-stable-rt.git/tag/?h=v5.4.74-rt41-rebase and untar (~900MB of sources) and cd
    \item \texttt{cp /boot/config-\$(uname -r) .config}
    \item \texttt{make olddefconfig}
    \item \texttt{make -j \$(nproc) deb-pkg}
    \item Wait ~4Hrs
    \item Install the deb pkgs and reboot
\end{itemize}

\section{Emulating the mobile platform}

Due to logistics issues caused by the COVID-19 pandemic, the IRAlab team has not been able to deploy the Otto mobile platform for several months: this posed a serious problem since we could not use it for developing the \texttt{a\_otto} adapter.

To work around this problem, we've set out to simulate Otto's communication stack by using an ESP8266 (NodeMCU "amica" 1.0) and providing dummy data to the demo's odometry module for testing. 

This eventually caused a deluge of issues since the two platforms behave in slightly different ways, and it was a huge setback.

\section{External impact on RT performance}

\subsection{Processor features and hardware components}
\begin{itemize}
    \item Caches
    \item CPU Threads (SMT)
    \item CPU Cores (SMP)
    \item Intel Management Engine (ME) and AMD Secure Platform (PSP)
    \item Power saving states, suspension, hybernation and clock boost
\end{itemize}

CPUSETS solve the problem of SMP (and SMT, to some extent) by locking specific processes (as in thread groups) to a partition of the available processors (or cores).

\subsection{Filesystem and I/O performance}

% Tail
\printbibliography
\end{document}